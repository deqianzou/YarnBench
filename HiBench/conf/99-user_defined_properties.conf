#======================================================
# Mandatory settings
#======================================================

# Hadoop home
hibench.hadoop.home		/home/admin/hadoop-2.7.1

# Spark home
hibench.spark.home		/home/admin/spark-1.6.1-bin-hadoop2.6

# HDFS master, set according to hdfs-site.xml
hibench.hdfs.master		hdfs://host6:9100

# Spark master
#   standalone mode: `spark://xxx:7077`
#   YARN mode: `yarn-client`
#   unset: fallback to `local[1]`
hibench.spark.master	yarn-client

#======================================================
# Not mandatory but important settings
#======================================================

# `hibench.hadoop.executable` is used to auto probe hadoop version and
# hadoop release, which is critical for further configurations. Most
# cases `hadoop` executable be placed under HADOOP_HOME/bin. However,
# in some cases such as CDH?/MR1, it must be explicitly defined:

hibench.hadoop.executable	${hibench.hadoop.home}/bin/hadoop

# `hibench.hadoop.version` will be auto probed according to
# `hibench.hadoop.executable` version information report. However, for
# CDH release, both `hadoop version` of MR1 and MR2 will produce same
# report, which can't probe right MR versions. You'll need to
# explicitly define MR versions here.

#hibench.hadoop.version		hadoop1

# `hibench.spark.version` is used to choose which sparkbench workload
# jar. Mostly situation it'll be auto probed. Please override if spark
# version is not probed correctly. For spark version after 1.6, please
# set it to spark1.6
# Note, supported values: `spark1.2` to `spark1.6`

hibench.spark.version          spark1.6

#======================================================
# Optional settings
#======================================================

##queue parameter
hibench.default.queue                   default        

# Important parameters
#---------------------

##Docker suooport configuration

hibench.default.docker.executor         1
hibench.default.docker.image.key        yarn.nodemanager.docker-container-executor.image-name
hibench.default.docker.iamge.value      sequenceiq/hadoop-docker:2.4.1



# execute parallelism settings
hibench.default.map.parallelism		12
hibench.default.shuffle.parallelism 2	

# YARN resource configuration
hibench.yarn.executor.num	        5
hibench.yarn.executor.cores	        8

# Spark only properties
#----------------------

spark.eventLog.enabled           true
spark.local.dir                  hdfs://host6:9100/spark
spark.eventLog.dir               hdfs://host6:9100/spark/eventlog
spark.history.fs.logDirectory    hdfs://host6:9100/spark/spark-history

# executor/driver memory in standalone & YARN mode
spark.executor.memory            15G
spark.driver.memory              3G
####spark.dynamicAllocation.enabled   true
####spark.shuffle.service.enabled     true

# Compression
spark.rdd.compress            false
# compression codec: lz4, lzf, snappy, put class path here accordingly.
spark.shuffle.compress		  false
spark.broadcast.compress	  false
spark.io.compression.codec    org.apache.spark.io.SnappyCompressionCodec 

# Akka
spark.akka.frameSize          1000
spark.akka.timeout            600
#network
spark.network.timeout         3600
# mllib will use KyroSerializer, ensure the buffer is large enough
spark.kryoserializer.buffer.mb	 2000

# Data scale, Compression profile selection
#------------------------------------------

# Data scale profile: tiny, small, large, ..., defined in 10-data-scale-profile.conf
hibench.scale.profile  	        large
# Compression options selection: enable, disable
hibench.compress.profile	 	disable
# Compression codec profile selection:	 snappy, lzo, default
hibench.compress.codec.profile		snappy

# Streaming settings
#-------------------
# Available benchname: identity sample project grep wordcount distinctcount statistics
hibench.streamingbench.benchname	identity

# data scale of streaming data
hibench.streamingbench.scale.profile    ${hibench.scale.profile}

# zookeeper host:port of kafka cluster
#example         hostname:2181
hibench.streamingbench.zookeeper.host	HOSTNAME:HOSTPORT

# Kafka broker lists, used for direct mode, written in mode "host:port,host:port,..."
#example         hostname:9092
hibench.streamingbench.brokerList	HOSTNAME:HOSTPORT

# Storm home
hibench.streamingbench.storm.home	/PATH/TO/STORM/HOME

# Kafka home
hibench.streamingbench.kafka.home	/PATH/TO/KAFKA/HOME

#Cluster config
# nimbus of storm cluster
hibench.streamingbench.storm.nimbus	HOSTNAME_TO_STORM_NIMBUS

# partition size settings
hibench.streamingbench.partitions	1
